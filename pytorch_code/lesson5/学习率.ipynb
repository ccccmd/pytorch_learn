{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "学习率调整  \n",
    "学习率控制更新的步伐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_LRScheduler  \n",
    "主要属性：  \n",
    "* optimizer:关联的优化器  \n",
    "* last_epoch:记录epoch数  \n",
    "* base_lrs:记录初始学习率  \n",
    "\n",
    "主要方法：  \n",
    "* step():更新下一个epoch的学习率  \n",
    "* get_lr():虚函数，计算下一个epoch的学习率  \n",
    "\n",
    "scheduler.step要放到epoch的循环中去，不要放到iteration中去\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)\n",
    "LR = 0.1 \n",
    "iteration = 10\n",
    "max_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn((1), requires_grad=True)\n",
    "target = torch.zeros((1))\n",
    "optimizer = optim.SGD([weights], lr=LR, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调整策略  \n",
    "1.StepLR  \n",
    "功能：等间隔调整学习率  \n",
    "主要参数：  \n",
    "* step_size:调整间隔数  \n",
    "* gamma:调整系数  \n",
    "\n",
    "调整方式:lr = lr * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_lr = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Step LR Scheduler')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.MultiStepLR  \n",
    "功能：按给定间隔调整学习率  \n",
    "主要参数：  \n",
    "* milestones:设定调整时刻数  \n",
    "* gamma:调整系数  \n",
    "\n",
    "调整方式:lr = lr * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "milestones = [50, 125, 160]\n",
    "scheduler_lr = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Multi Step LR Scheduler\\nmilestones:{}'.format(milestones))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.ExponentialLR  \n",
    "功能：按指数衰减调整学习率  \n",
    "主要参数：  \n",
    "* gamma:指数的底  \n",
    "\n",
    "调整方式:lr = lr * gamma \\** epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_lr = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Exponentila LR Scheduler\\ngamma:{}'.format(0.95))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.CosineAnnealingLR  \n",
    "功能：余弦周期调整学习率  \n",
    "主要参数：  \n",
    "* T_max:下降周期  \n",
    "* eta_min:学习率下限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 50\n",
    "scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=0.)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='CosineAnnealingLR Scheduler\\nT_max:{}'.format(t_max))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.ReduceLRonPlateau  \n",
    "功能：监控指标，当指标不再变化则调整  \n",
    "主要参数：  \n",
    "* mode:min/max两种模式  \n",
    "* factor:调整系数  \n",
    "* patience:\"耐心\"，接受几次不变化  \n",
    "* cooldown:\"冷却时间\"，停止监控一段时间  \n",
    "* verbose:是否打印日志  \n",
    "* min_lr:学习率下限  \n",
    "* eps:学习率衰减最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\nEpoch    38: reducing learning rate of group 0 to 1.0000e-03.\nEpoch    59: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "loss_value = 0.5\n",
    "accuracy = 0.9\n",
    "factor = 0.1\n",
    "mode='min'\n",
    "patience = 10\n",
    "cooldown=10\n",
    "min_lr = 1e-4\n",
    "verbose = True\n",
    "scheduler_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=factor, mode=mode, patience=patience, cooldown=cooldown, min_lr=min_lr, verbose=True)\n",
    "for epoch in range(max_epoch):\n",
    "    for i in range(iteration):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch == 5:\n",
    "        loss_value = 0.4\n",
    "    scheduler_lr.step(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
