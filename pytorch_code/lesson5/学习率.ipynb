{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "学习率调整  \n",
    "学习率控制更新的步伐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_LRScheduler  \n",
    "主要属性：  \n",
    "* optimizer:关联的优化器  \n",
    "* last_epoch:记录epoch数  \n",
    "* base_lrs:记录初始学习率  \n",
    "\n",
    "主要方法：  \n",
    "* step():更新下一个epoch的学习率  \n",
    "* get_lr():虚函数，计算下一个epoch的学习率  \n",
    "\n",
    "scheduler.step要放到epoch的循环中去，不要放到iteration中去\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)\n",
    "LR = 0.1 \n",
    "iteration = 10\n",
    "max_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn((1), requires_grad=True)\n",
    "target = torch.zeros((1))\n",
    "optimizer = optim.SGD([weights], lr=LR, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调整策略  \n",
    "1.StepLR  \n",
    "功能：等间隔调整学习率  \n",
    "主要参数：  \n",
    "* step_size:调整间隔数  \n",
    "* gamma:调整系数  \n",
    "\n",
    "调整方式:lr = lr * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_lr = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Step LR Scheduler')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.MultiStepLR  \n",
    "功能：按给定间隔调整学习率  \n",
    "主要参数：  \n",
    "* milestones:设定调整时刻数  \n",
    "* gamma:调整系数  \n",
    "\n",
    "调整方式:lr = lr * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "milestones = [50, 125, 160]\n",
    "scheduler_lr = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Multi Step LR Scheduler\\nmilestones:{}'.format(milestones))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.ExponentialLR  \n",
    "功能：按指数衰减调整学习率  \n",
    "主要参数：  \n",
    "* gamma:指数的底  \n",
    "\n",
    "调整方式:lr = lr * gamma \\** epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_lr = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='Exponentila LR Scheduler\\ngamma:{}'.format(0.95))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.CosineAnnealingLR  \n",
    "功能：余弦周期调整学习率  \n",
    "主要参数：  \n",
    "* T_max:下降周期  \n",
    "* eta_min:学习率下限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 50\n",
    "scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=0.)\n",
    "lr_list, epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    lr_list.append(scheduler_lr.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    for i in range(iteration):\n",
    "        loss = torch.pow((weights - target), 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler_lr.step()\n",
    "plt.plot(epoch_list, lr_list, label='CosineAnnealingLR Scheduler\\nT_max:{}'.format(t_max))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.ReduceLRonPlateau  \n",
    "功能：监控指标，当指标不再变化则调整  \n",
    "主要参数：  \n",
    "* mode:min/max两种模式  \n",
    "* factor:调整系数  \n",
    "* patience:\"耐心\"，接受几次不变化  \n",
    "* cooldown:\"冷却时间\"，停止监控一段时间  \n",
    "* verbose:是否打印日志  \n",
    "* min_lr:学习率下限  \n",
    "* eps:学习率衰减最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\nEpoch    38: reducing learning rate of group 0 to 1.0000e-03.\nEpoch    59: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "loss_value = 0.5\n",
    "accuracy = 0.9\n",
    "factor = 0.1\n",
    "mode='min'\n",
    "patience = 10\n",
    "cooldown = 10\n",
    "min_lr = 1e-4\n",
    "verbose = True\n",
    "scheduler_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=factor, mode=mode, patience=patience, cooldown=cooldown, min_lr=min_lr, verbose=True)\n",
    "for epoch in range(max_epoch):\n",
    "    for i in range(iteration):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch == 5:\n",
    "        loss_value = 0.4\n",
    "    scheduler_lr.step(loss_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.LambadaLR  \n",
    "功能：自定义调整策略  \n",
    "主要参数：  \n",
    "* lr_lambda:function or list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0, lr:[0.1, 0.095]\nepoch:    1, lr:[0.1, 0.09025]\nepoch:    2, lr:[0.1, 0.0857375]\nepoch:    3, lr:[0.1, 0.081450625]\nepoch:    4, lr:[0.1, 0.07737809374999999]\nepoch:    5, lr:[0.1, 0.07350918906249998]\nepoch:    6, lr:[0.1, 0.06983372960937498]\nepoch:    7, lr:[0.1, 0.06634204312890622]\nepoch:    8, lr:[0.1, 0.0630249409724609]\nepoch:    9, lr:[0.1, 0.05987369392383787]\nepoch:   10, lr:[0.1, 0.05688000922764597]\nepoch:   11, lr:[0.1, 0.05403600876626367]\nepoch:   12, lr:[0.1, 0.051334208327950485]\nepoch:   13, lr:[0.1, 0.04876749791155296]\nepoch:   14, lr:[0.1, 0.046329123015975304]\nepoch:   15, lr:[0.1, 0.04401266686517654]\nepoch:   16, lr:[0.1, 0.04181203352191771]\nepoch:   17, lr:[0.1, 0.039721431845821824]\nepoch:   18, lr:[0.1, 0.03773536025353073]\nepoch:   19, lr:[0.010000000000000002, 0.03584859224085419]\nepoch:   20, lr:[0.010000000000000002, 0.03405616262881148]\nepoch:   21, lr:[0.010000000000000002, 0.0323533544973709]\nepoch:   22, lr:[0.010000000000000002, 0.03073568677250236]\nepoch:   23, lr:[0.010000000000000002, 0.02919890243387724]\nepoch:   24, lr:[0.010000000000000002, 0.027738957312183378]\nepoch:   25, lr:[0.010000000000000002, 0.026352009446574204]\nepoch:   26, lr:[0.010000000000000002, 0.025034408974245494]\nepoch:   27, lr:[0.010000000000000002, 0.023782688525533217]\nepoch:   28, lr:[0.010000000000000002, 0.022593554099256556]\nepoch:   29, lr:[0.010000000000000002, 0.02146387639429373]\nepoch:   30, lr:[0.010000000000000002, 0.02039068257457904]\nepoch:   31, lr:[0.010000000000000002, 0.019371148445850087]\nepoch:   32, lr:[0.010000000000000002, 0.018402591023557582]\nepoch:   33, lr:[0.010000000000000002, 0.017482461472379703]\nepoch:   34, lr:[0.010000000000000002, 0.016608338398760716]\nepoch:   35, lr:[0.010000000000000002, 0.01577792147882268]\nepoch:   36, lr:[0.010000000000000002, 0.014989025404881546]\nepoch:   37, lr:[0.010000000000000002, 0.014239574134637467]\nepoch:   38, lr:[0.010000000000000002, 0.013527595427905593]\nepoch:   39, lr:[0.0010000000000000002, 0.012851215656510312]\nepoch:   40, lr:[0.0010000000000000002, 0.012208654873684797]\nepoch:   41, lr:[0.0010000000000000002, 0.011598222130000557]\nepoch:   42, lr:[0.0010000000000000002, 0.011018311023500529]\nepoch:   43, lr:[0.0010000000000000002, 0.010467395472325502]\nepoch:   44, lr:[0.0010000000000000002, 0.009944025698709225]\nepoch:   45, lr:[0.0010000000000000002, 0.009446824413773765]\nepoch:   46, lr:[0.0010000000000000002, 0.008974483193085076]\nepoch:   47, lr:[0.0010000000000000002, 0.00852575903343082]\nepoch:   48, lr:[0.0010000000000000002, 0.00809947108175928]\nepoch:   49, lr:[0.0010000000000000002, 0.007694497527671315]\nepoch:   50, lr:[0.0010000000000000002, 0.007309772651287749]\nepoch:   51, lr:[0.0010000000000000002, 0.006944284018723362]\nepoch:   52, lr:[0.0010000000000000002, 0.0065970698177871935]\nepoch:   53, lr:[0.0010000000000000002, 0.006267216326897833]\nepoch:   54, lr:[0.0010000000000000002, 0.005953855510552941]\nepoch:   55, lr:[0.0010000000000000002, 0.005656162735025293]\nepoch:   56, lr:[0.0010000000000000002, 0.005373354598274029]\nepoch:   57, lr:[0.0010000000000000002, 0.005104686868360327]\nepoch:   58, lr:[0.0010000000000000002, 0.004849452524942311]\nepoch:   59, lr:[0.00010000000000000003, 0.004606979898695194]\nepoch:   60, lr:[0.00010000000000000003, 0.004376630903760435]\nepoch:   61, lr:[0.00010000000000000003, 0.004157799358572413]\nepoch:   62, lr:[0.00010000000000000003, 0.003949909390643792]\nepoch:   63, lr:[0.00010000000000000003, 0.003752413921111602]\nepoch:   64, lr:[0.00010000000000000003, 0.003564793225056022]\nepoch:   65, lr:[0.00010000000000000003, 0.0033865535638032207]\nepoch:   66, lr:[0.00010000000000000003, 0.0032172258856130592]\nepoch:   67, lr:[0.00010000000000000003, 0.0030563645913324064]\nepoch:   68, lr:[0.00010000000000000003, 0.002903546361765786]\nepoch:   69, lr:[0.00010000000000000003, 0.0027583690436774966]\nepoch:   70, lr:[0.00010000000000000003, 0.0026204505914936217]\nepoch:   71, lr:[0.00010000000000000003, 0.0024894280619189406]\nepoch:   72, lr:[0.00010000000000000003, 0.0023649566588229936]\nepoch:   73, lr:[0.00010000000000000003, 0.0022467088258818434]\nepoch:   74, lr:[0.00010000000000000003, 0.002134373384587751]\nepoch:   75, lr:[0.00010000000000000003, 0.0020276547153583635]\nepoch:   76, lr:[0.00010000000000000003, 0.0019262719795904452]\nepoch:   77, lr:[0.00010000000000000003, 0.001829958380610923]\nepoch:   78, lr:[0.00010000000000000003, 0.0017384604615803768]\nepoch:   79, lr:[1.0000000000000003e-05, 0.001651537438501358]\nepoch:   80, lr:[1.0000000000000003e-05, 0.00156896056657629]\nepoch:   81, lr:[1.0000000000000003e-05, 0.0014905125382474755]\nepoch:   82, lr:[1.0000000000000003e-05, 0.0014159869113351015]\nepoch:   83, lr:[1.0000000000000003e-05, 0.0013451875657683465]\nepoch:   84, lr:[1.0000000000000003e-05, 0.001277928187479929]\nepoch:   85, lr:[1.0000000000000003e-05, 0.0012140317781059325]\nepoch:   86, lr:[1.0000000000000003e-05, 0.0011533301892006358]\nepoch:   87, lr:[1.0000000000000003e-05, 0.001095663679740604]\nepoch:   88, lr:[1.0000000000000003e-05, 0.0010408804957535737]\nepoch:   89, lr:[1.0000000000000003e-05, 0.000988836470965895]\nepoch:   90, lr:[1.0000000000000003e-05, 0.0009393946474176001]\nepoch:   91, lr:[1.0000000000000003e-05, 0.0008924249150467202]\nepoch:   92, lr:[1.0000000000000003e-05, 0.0008478036692943841]\nepoch:   93, lr:[1.0000000000000003e-05, 0.0008054134858296649]\nepoch:   94, lr:[1.0000000000000003e-05, 0.0007651428115381816]\nepoch:   95, lr:[1.0000000000000003e-05, 0.0007268856709612725]\nepoch:   96, lr:[1.0000000000000003e-05, 0.0006905413874132089]\nepoch:   97, lr:[1.0000000000000003e-05, 0.0006560143180425484]\nepoch:   98, lr:[1.0000000000000003e-05, 0.0006232136021404209]\nepoch:   99, lr:[1.0000000000000004e-06, 0.0005920529220333997]\nepoch:  100, lr:[1.0000000000000004e-06, 0.0005624502759317298]\nepoch:  101, lr:[1.0000000000000004e-06, 0.0005343277621351433]\nepoch:  102, lr:[1.0000000000000004e-06, 0.0005076113740283861]\nepoch:  103, lr:[1.0000000000000004e-06, 0.00048223080532696673]\nepoch:  104, lr:[1.0000000000000004e-06, 0.0004581192650606184]\nepoch:  105, lr:[1.0000000000000004e-06, 0.00043521330180758743]\nepoch:  106, lr:[1.0000000000000004e-06, 0.00041345263671720806]\nepoch:  107, lr:[1.0000000000000004e-06, 0.0003927800048813476]\nepoch:  108, lr:[1.0000000000000004e-06, 0.00037314100463728026]\nepoch:  109, lr:[1.0000000000000004e-06, 0.00035448395440541624]\nepoch:  110, lr:[1.0000000000000004e-06, 0.0003367597566851454]\nepoch:  111, lr:[1.0000000000000004e-06, 0.0003199217688508881]\nepoch:  112, lr:[1.0000000000000004e-06, 0.0003039256804083437]\nepoch:  113, lr:[1.0000000000000004e-06, 0.0002887293963879265]\nepoch:  114, lr:[1.0000000000000004e-06, 0.00027429292656853016]\nepoch:  115, lr:[1.0000000000000004e-06, 0.00026057828024010366]\nepoch:  116, lr:[1.0000000000000004e-06, 0.0002475493662280985]\nepoch:  117, lr:[1.0000000000000004e-06, 0.00023517189791669353]\nepoch:  118, lr:[1.0000000000000004e-06, 0.0002234133030208588]\nepoch:  119, lr:[1.0000000000000005e-07, 0.00021224263786981585]\nepoch:  120, lr:[1.0000000000000005e-07, 0.00020163050597632508]\nepoch:  121, lr:[1.0000000000000005e-07, 0.0001915489806775088]\nepoch:  122, lr:[1.0000000000000005e-07, 0.00018197153164363337]\nepoch:  123, lr:[1.0000000000000005e-07, 0.00017287295506145168]\nepoch:  124, lr:[1.0000000000000005e-07, 0.00016422930730837908]\nepoch:  125, lr:[1.0000000000000005e-07, 0.00015601784194296014]\nepoch:  126, lr:[1.0000000000000005e-07, 0.00014821694984581212]\nepoch:  127, lr:[1.0000000000000005e-07, 0.0001408061023535215]\nepoch:  128, lr:[1.0000000000000005e-07, 0.00013376579723584542]\nepoch:  129, lr:[1.0000000000000005e-07, 0.00012707750737405313]\nepoch:  130, lr:[1.0000000000000005e-07, 0.00012072363200535048]\nepoch:  131, lr:[1.0000000000000005e-07, 0.00011468745040508295]\nepoch:  132, lr:[1.0000000000000005e-07, 0.0001089530778848288]\nepoch:  133, lr:[1.0000000000000005e-07, 0.00010350542399058736]\nepoch:  134, lr:[1.0000000000000005e-07, 9.833015279105799e-05]\nepoch:  135, lr:[1.0000000000000005e-07, 9.341364515150508e-05]\nepoch:  136, lr:[1.0000000000000005e-07, 8.874296289392982e-05]\nepoch:  137, lr:[1.0000000000000005e-07, 8.430581474923332e-05]\nepoch:  138, lr:[1.0000000000000005e-07, 8.009052401177165e-05]\nepoch:  139, lr:[1.0000000000000004e-08, 7.608599781118307e-05]\nepoch:  140, lr:[1.0000000000000004e-08, 7.228169792062392e-05]\nepoch:  141, lr:[1.0000000000000004e-08, 6.866761302459272e-05]\nepoch:  142, lr:[1.0000000000000004e-08, 6.523423237336307e-05]\nepoch:  143, lr:[1.0000000000000004e-08, 6.197252075469492e-05]\nepoch:  144, lr:[1.0000000000000004e-08, 5.8873894716960165e-05]\nepoch:  145, lr:[1.0000000000000004e-08, 5.593019998111216e-05]\nepoch:  146, lr:[1.0000000000000004e-08, 5.313368998205655e-05]\nepoch:  147, lr:[1.0000000000000004e-08, 5.0477005482953716e-05]\nepoch:  148, lr:[1.0000000000000004e-08, 4.795315520880603e-05]\nepoch:  149, lr:[1.0000000000000004e-08, 4.555549744836572e-05]\nepoch:  150, lr:[1.0000000000000004e-08, 4.327772257594744e-05]\nepoch:  151, lr:[1.0000000000000004e-08, 4.1113836447150066e-05]\nepoch:  152, lr:[1.0000000000000004e-08, 3.905814462479256e-05]\nepoch:  153, lr:[1.0000000000000004e-08, 3.710523739355293e-05]\nepoch:  154, lr:[1.0000000000000004e-08, 3.524997552387528e-05]\nepoch:  155, lr:[1.0000000000000004e-08, 3.3487476747681514e-05]\nepoch:  156, lr:[1.0000000000000004e-08, 3.181310291029744e-05]\nepoch:  157, lr:[1.0000000000000004e-08, 3.0222447764782564e-05]\nepoch:  158, lr:[1.0000000000000004e-08, 2.8711325376543437e-05]\nepoch:  159, lr:[1.0000000000000005e-09, 2.7275759107716264e-05]\nepoch:  160, lr:[1.0000000000000005e-09, 2.5911971152330445e-05]\nepoch:  161, lr:[1.0000000000000005e-09, 2.4616372594713925e-05]\nepoch:  162, lr:[1.0000000000000005e-09, 2.3385553964978226e-05]\nepoch:  163, lr:[1.0000000000000005e-09, 2.2216276266729317e-05]\nepoch:  164, lr:[1.0000000000000005e-09, 2.110546245339285e-05]\nepoch:  165, lr:[1.0000000000000005e-09, 2.0050189330723204e-05]\nepoch:  166, lr:[1.0000000000000005e-09, 1.9047679864187045e-05]\nepoch:  167, lr:[1.0000000000000005e-09, 1.809529587097769e-05]\nepoch:  168, lr:[1.0000000000000005e-09, 1.7190531077428805e-05]\nepoch:  169, lr:[1.0000000000000005e-09, 1.6331004523557364e-05]\nepoch:  170, lr:[1.0000000000000005e-09, 1.5514454297379498e-05]\nepoch:  171, lr:[1.0000000000000005e-09, 1.4738731582510519e-05]\nepoch:  172, lr:[1.0000000000000005e-09, 1.4001795003384993e-05]\nepoch:  173, lr:[1.0000000000000005e-09, 1.3301705253215743e-05]\nepoch:  174, lr:[1.0000000000000005e-09, 1.2636619990554954e-05]\nepoch:  175, lr:[1.0000000000000005e-09, 1.2004788991027206e-05]\nepoch:  176, lr:[1.0000000000000005e-09, 1.1404549541475845e-05]\nepoch:  177, lr:[1.0000000000000005e-09, 1.0834322064402054e-05]\nepoch:  178, lr:[1.0000000000000005e-09, 1.029260596118195e-05]\nepoch:  179, lr:[1.0000000000000006e-10, 9.777975663122852e-06]\nepoch:  180, lr:[1.0000000000000006e-10, 9.28907687996671e-06]\nepoch:  181, lr:[1.0000000000000006e-10, 8.824623035968373e-06]\nepoch:  182, lr:[1.0000000000000006e-10, 8.383391884169954e-06]\nepoch:  183, lr:[1.0000000000000006e-10, 7.964222289961456e-06]\nepoch:  184, lr:[1.0000000000000006e-10, 7.566011175463383e-06]\nepoch:  185, lr:[1.0000000000000006e-10, 7.187710616690214e-06]\nepoch:  186, lr:[1.0000000000000006e-10, 6.828325085855702e-06]\nepoch:  187, lr:[1.0000000000000006e-10, 6.486908831562916e-06]\nepoch:  188, lr:[1.0000000000000006e-10, 6.16256338998477e-06]\nepoch:  189, lr:[1.0000000000000006e-10, 5.854435220485532e-06]\nepoch:  190, lr:[1.0000000000000006e-10, 5.5617134594612554e-06]\nepoch:  191, lr:[1.0000000000000006e-10, 5.283627786488193e-06]\nepoch:  192, lr:[1.0000000000000006e-10, 5.0194463971637825e-06]\nepoch:  193, lr:[1.0000000000000006e-10, 4.768474077305593e-06]\nepoch:  194, lr:[1.0000000000000006e-10, 4.5300503734403135e-06]\nepoch:  195, lr:[1.0000000000000006e-10, 4.3035478547682975e-06]\nepoch:  196, lr:[1.0000000000000006e-10, 4.088370462029883e-06]\nepoch:  197, lr:[1.0000000000000006e-10, 3.883951938928388e-06]\nepoch:  198, lr:[1.0000000000000006e-10, 3.6897543419819688e-06]\nepoch:  199, lr:[1.0000000000000006e-11, 3.5052666248828703e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenmengda/opt/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    }
   ],
   "source": [
    "lr_init = 0.1\n",
    "weights_1 = torch.randn((6, 3, 5, 5))\n",
    "weights_2 = torch.ones((4, 4, 5, 5))\n",
    "optimizer = optim.SGD([{'params':weights_2}, {'params':weights_1}], lr=lr_init)\n",
    "lambda1 = lambda epoch: 0.1 ** (epoch // 20)\n",
    "lambda2 = lambda epoch: 0.95 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "lr_list,epoch_list = list(), list()\n",
    "for epoch in range(max_epoch):\n",
    "    for i in range(iteration):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    lr_list.append(scheduler.get_lr())\n",
    "    epoch_list.append(epoch)\n",
    "    print('epoch:{:5d}, lr:{}'.format(epoch, scheduler.get_lr()))\n",
    "plt.plot(epoch_list, [i[0] for i in lr_list], label='lambda 1')\n",
    "plt.plot(epoch_list, [i[1] for i in lr_list], label='lambda 2')\n",
    "plt.plot(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"LambdaLR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调整小结  \n",
    "1. 有序调整：Step、MultiStep、Exponential和CosineAnnealing  \n",
    "2. 自适应调整：ReduceLROnPleateau  \n",
    "3. 自定义调整：Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
