{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Batch Normalization:批标准化  \n",
    "批:一批数据，通常为mini-batch  \n",
    "标准化:0均值，1方差  \n",
    "优点：可以用更大学习率，加速模型收敛  \n",
    "可以不用精心设计权值初始化  \n",
    "可以不用dropout或较小的dropout  \n",
    "可以不用L2或者较小的weight decay  \n",
    "可以不用LRN(local response normalization)  \n",
    "\n",
    "初衷是解决ICS问题，防止梯度消失和梯度爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers=100):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(neural_num) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for (i, linear), bn in zip(enumerate(self.linears), self.bns):\n",
    "            x = linear(x)\n",
    "            x = bn(x)\n",
    "            x = torch.relu(x)\n",
    "            if torch.isnan(x.std()):\n",
    "                print('output is nan in {} layers'.format(i))\n",
    "                break\n",
    "            print('layers:{}, std:{}'.format(i, x.std().item()))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.5789448022842407\nlayers:1, std:0.5849304795265198\nlayers:2, std:0.5713231563568115\nlayers:3, std:0.5765722990036011\nlayers:4, std:0.57011479139328\nlayers:5, std:0.5769959092140198\nlayers:6, std:0.576199471950531\nlayers:7, std:0.5761939883232117\nlayers:8, std:0.586577832698822\nlayers:9, std:0.5730356574058533\nlayers:10, std:0.5820686221122742\nlayers:11, std:0.58272385597229\nlayers:12, std:0.5818347334861755\nlayers:13, std:0.577144980430603\nlayers:14, std:0.5806982517242432\nlayers:15, std:0.5770207047462463\nlayers:16, std:0.5772503018379211\nlayers:17, std:0.5789951682090759\nlayers:18, std:0.5796297192573547\nlayers:19, std:0.5861504673957825\nlayers:20, std:0.5751968622207642\nlayers:21, std:0.5832950472831726\nlayers:22, std:0.5741185545921326\nlayers:23, std:0.5780283808708191\nlayers:24, std:0.5823552012443542\nlayers:25, std:0.5857341885566711\nlayers:26, std:0.5807106494903564\nlayers:27, std:0.5796788334846497\nlayers:28, std:0.5729328393936157\nlayers:29, std:0.5814088582992554\nlayers:30, std:0.5805947780609131\nlayers:31, std:0.5837904214859009\nlayers:32, std:0.5877171158790588\nlayers:33, std:0.5866210460662842\nlayers:34, std:0.5790449976921082\nlayers:35, std:0.5847129225730896\nlayers:36, std:0.5860323905944824\nlayers:37, std:0.5825194120407104\nlayers:38, std:0.5852624177932739\nlayers:39, std:0.5714762806892395\nlayers:40, std:0.5841423869132996\nlayers:41, std:0.5792348980903625\nlayers:42, std:0.577506959438324\nlayers:43, std:0.5829699039459229\nlayers:44, std:0.5872020125389099\nlayers:45, std:0.5766071677207947\nlayers:46, std:0.5822429060935974\nlayers:47, std:0.5798041820526123\nlayers:48, std:0.5827981233596802\nlayers:49, std:0.5888009667396545\nlayers:50, std:0.578717052936554\nlayers:51, std:0.581953763961792\nlayers:52, std:0.5879130363464355\nlayers:53, std:0.5787878036499023\nlayers:54, std:0.5881322622299194\nlayers:55, std:0.5853919982910156\nlayers:56, std:0.5808285474777222\nlayers:57, std:0.5721958875656128\nlayers:58, std:0.5785861611366272\nlayers:59, std:0.5619706511497498\nlayers:60, std:0.5793201327323914\nlayers:61, std:0.5810468792915344\nlayers:62, std:0.5709307789802551\nlayers:63, std:0.585089921951294\nlayers:64, std:0.5752407312393188\nlayers:65, std:0.5737406611442566\nlayers:66, std:0.5844172835350037\nlayers:67, std:0.5740854144096375\nlayers:68, std:0.5843883752822876\nlayers:69, std:0.5827963948249817\nlayers:70, std:0.5824385285377502\nlayers:71, std:0.5773729681968689\nlayers:72, std:0.5830615758895874\nlayers:73, std:0.5779255628585815\nlayers:74, std:0.5823503136634827\nlayers:75, std:0.5830780863761902\nlayers:76, std:0.5747953653335571\nlayers:77, std:0.5800734758377075\nlayers:78, std:0.5780124664306641\nlayers:79, std:0.580996572971344\nlayers:80, std:0.58475661277771\nlayers:81, std:0.5807645916938782\nlayers:82, std:0.5793624520301819\nlayers:83, std:0.588838517665863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:84, std:0.574193000793457\nlayers:85, std:0.5727437734603882\nlayers:86, std:0.5731297135353088\nlayers:87, std:0.5699641108512878\nlayers:88, std:0.5749478340148926\nlayers:89, std:0.5808009505271912\nlayers:90, std:0.5788522362709045\nlayers:91, std:0.5733445286750793\nlayers:92, std:0.5718269944190979\nlayers:93, std:0.5795019865036011\nlayers:94, std:0.5751311779022217\nlayers:95, std:0.5843010544776917\nlayers:96, std:0.5802408456802368\nlayers:97, std:0.5779411792755127\nlayers:98, std:0.5809749960899353\nlayers:99, std:0.58396977186203\ntensor([[0.0000, 0.4899, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.1303, 0.6495, 0.7949,  ..., 0.0777, 0.8714, 0.0000],\n        [0.2747, 0.0000, 0.0000,  ..., 0.2426, 1.1777, 0.0909],\n        ...,\n        [0.0000, 1.4464, 1.2353,  ..., 0.0000, 1.1637, 0.2478],\n        [0.0000, 2.0306, 0.1914,  ..., 0.0000, 0.0000, 2.5218],\n        [0.3302, 0.0000, 0.0000,  ..., 0.1338, 0.0000, 1.1510]],\n       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "neural_num = 256\n",
    "layer_nums = 100\n",
    "batch_size = 16\n",
    "net = MLP(neural_num, layer_nums)\n",
    "# net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_num))\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_BatchNorm  \n",
    "* nn.BatchNorm1d  \n",
    "* nn.BatchNorm2d  \n",
    "* nn.BatchNorm3d  \n",
    "\n",
    "参数：  \n",
    "* num_features:一个样本特征数量（最重要）  \n",
    "* eps:分母修正项  \n",
    "* momentum:指数加权平均估计当前mean/var  \n",
    "* affine:是否需要affine transform  \n",
    "* track_running_stats:是训练状态，还是测试状态  \n",
    "\n",
    "主要属性：  \n",
    "* running_mean:均值  \n",
    "* running_var:方差  \n",
    "* weight:affine transform中的gamma  \n",
    "* bias:affine transform中的beta  \n",
    "训练：均值和方差采用指数加权平均计算  \n",
    "测试：当前统计值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
